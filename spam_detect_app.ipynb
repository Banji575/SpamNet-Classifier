{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6de66ad-05da-43d2-bdeb-79935a36a601",
   "metadata": {},
   "source": [
    "# Spam Detection Model Preparation\r\n",
    "\r\n",
    "The following section of the code serves as the initial step in preparing a spam detection model. It includes the import of necessary libraries, the definition of a function for data extraction and preprocessing, and the reading of the dataset from a local zip file.\r\n",
    "\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "357519b5-ee72-4e87-a8a2-0b9e4cae9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation, machine learning, and file handling\n",
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Define a function to read data from a local zip file\n",
    "def read_data(local_zip_path):\n",
    "    # Determine the path for the extracted file\n",
    "    extracted_folder_path = os.path.splitext(local_zip_path)[0]\n",
    "    sms_collection_path = os.path.join(extracted_folder_path, 'SMSSpamCollection')\n",
    "\n",
    "    # Unzip the file if the directory does not exist\n",
    "    if not os.path.isdir(extracted_folder_path):\n",
    "        with ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "    # Read and split the data into labels and texts from the extracted file\n",
    "    labels, texts = [], []\n",
    "    with open(sms_collection_path, 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == 'spam' else 0)  # Convert labels to binary (spam: 1, ham: 0)\n",
    "            texts.append(text)  # Append text data to the texts list\n",
    "    return texts, labels\n",
    "\n",
    "# Specify the path to your local zip file containing the dataset\n",
    "local_zip_path = 'datasets/smsspamcollection.zip'\n",
    "texts, labels = read_data(local_zip_path)  # Read the dataset into texts and labels variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a05b9-ebd2-47dc-bfbc-09475f210fc8",
   "metadata": {},
   "source": [
    "# Text Tokenization and Sequence Padding\n",
    "\n",
    "After loading the dataset, the next step in text processing is to convert the raw text into a format that can be fed into a neural network. This involves tokenizing the text and padding the sequences to have uniform length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9be9eeb6-cc96-48e0-953f-ae05bb00e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 sentences, max length: 189\n",
      "Vocabulary size including padding token: 9010\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer which will be used to vectorize text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the texts - this will create a mapping between words and integers\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert the list of texts to sequences of integers\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to ensure that all sequences in a list have the same length\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "\n",
    "# Calculate the number of records and the maximum sequence length\n",
    "num_record = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num_record, max_seqlen))  # Print out the dataset stats\n",
    "\n",
    "# Define the number of classes for classification (spam or not spam)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Convert labels to one-hot encoding format required for training the neural network\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Create mappings from words to integers and vice versa, including padding token\n",
    "word2idx = tokenizer.word_index  # Dictionary mapping words to their integer representation\n",
    "idx2word = {v:k for k, v in word2idx.items()}  # Reverse mapping from integers to words\n",
    "word2idx[\"PAD\"] = 0  # Add padding token to the word index dictionary\n",
    "idx2word[0] = \"PAD\"  # Corresponding reverse mapping for padding\n",
    "\n",
    "# Determine the vocabulary size for further use in the model (including the padding token)\n",
    "vocab_size = len(word2idx)\n",
    "print('Vocabulary size including padding token: {:d}'.format(vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c592f-652c-4584-9d92-b7ff05d8cc05",
   "metadata": {},
   "source": [
    "# Dataset Splitting and Batching\n",
    "\n",
    "Once the text data is tokenized and padded, the next step is to create a dataset object that can be used to feed data into the neural network in batches. This involves splitting the dataset into training, validation, and test sets, and batching the data for efficient training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8451f0ac-d34c-4f2c-90c1-748ca43fa685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 512, Validation set size: 10\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow Dataset object which is suitable for feeding data into the model\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "\n",
    "# Shuffle the dataset to ensure the model gets data points in a random order during training\n",
    "dataset = dataset.shuffle(10000)\n",
    "\n",
    "# Define the sizes for the test and validation sets\n",
    "test_size = 512  # Size of the test set\n",
    "val_size = 10    # Size of the validation set\n",
    "print(f'Test set size: {test_size}, Validation set size: {val_size}') \n",
    "\n",
    "# Split the dataset into test, validation, and training sets\n",
    "test_dataset = dataset.take(test_size)  # Take 'test_size' records for the test set\n",
    "val_dataset = dataset.skip(test_size).take(val_size)  # Skip 'test_size' records and take the next 'val_size' for validation\n",
    "train_dataset = dataset.skip(test_size + val_size)  # Use the remaining data for training\n",
    "\n",
    "# Batch size is the number of samples that will be propagated through the network in one pass\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Batch the test, validation, and training sets with the defined batch size\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)  # Batch the test dataset and drop the last batch if it has fewer than BATCH_SIZE elements\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)    # Same for the validation dataset\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True) # Same for the training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7332dd8-7f00-45ff-a631-3c76d7f65355",
   "metadata": {},
   "source": [
    "# Building the Embedding Matrix\n",
    "\n",
    "An essential part of many natural language processing tasks is the use of word embeddings. The following code demonstrates how to build an embedding matrix using pre-trained embeddings from GloVe, which can then be used in the neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a499c97a-d957-44c5-802c-83674835878b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9010, 300)\n"
     ]
    }
   ],
   "source": [
    "# List available models in gensim - this line can be used to select an appropriate pre-trained embedding model\n",
    "api.info()['models'].keys()\n",
    "\n",
    "# Define a function to build an embedding matrix\n",
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
    "    # Check if the embedding matrix file exists to avoid redundant computations\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)  # Load the embedding matrix if it already exists\n",
    "    else:\n",
    "        vocab_size = len(word2idx)  # Get the size of the vocabulary\n",
    "        E = np.zeros((vocab_size, embedding_dim))  # Initialize the embedding matrix with zeros\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)  # Load pre-trained word vectors\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.get_vector(word)  # Assign the vector of each word in the vocabulary\n",
    "            except KeyError:  # If a word is not in the embedding model\n",
    "                pass  # Skip the word\n",
    "        np.save(embedding_file, E)  # Save the embedding matrix for future use\n",
    "    return E\n",
    "\n",
    "# Embedding dimensions and file specifications\n",
    "EMBEDDING_DIM = 300  # Dimension of the GloVe vectors\n",
    "DATA_DIR = \"data\"  # Directory to store data files\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")  # Path for the numpy file of the embedding matrix\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"  # The pre-trained GloVe model to use\n",
    "\n",
    "# Build the embedding matrix based on the current dataset\n",
    "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM, EMBEDDING_NUMPY_FILE)\n",
    "print(\"Embedding matrix shape:\", E.shape)  # Print the shape of the embedding matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b57b5-898f-402c-a1e5-65841c289ddd",
   "metadata": {},
   "source": [
    "# Defining and Training the Spam Classifier Model\n",
    "\n",
    "In this section, we define a custom neural network model for spam classification. The model is built using TensorFlow's Keras API, and it utilizes convolutional and dense layers for processing the input text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6aff6e16-a98c-4ebc-9ae6-2a9d3b891f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "39/39 [==============================] - 5s 108ms/step - loss: 0.8712 - accuracy: 0.7754\n",
      "Epoch 2/3\n",
      "39/39 [==============================] - 4s 101ms/step - loss: 0.1863 - accuracy: 0.9782\n",
      "Epoch 3/3\n",
      "39/39 [==============================] - 4s 105ms/step - loss: 0.0547 - accuracy: 0.9944\n",
      "Test accuracy: 1.000000\n",
      "Confusion matrix:\n",
      "[[438   0]\n",
      " [  0  74]]\n"
     ]
    }
   ],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, input_length, num_filters, kernel_sz, output_sz, run_mode, embedding_weights, **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        # Choose the embedding layer based on the run mode: 'scratch', 'vectorizer', or 'finetuning'\n",
    "        if run_mode == 'scratch':\n",
    "            # Create an embedding layer trainable from scratch\n",
    "            self.embedding = tf.keras.layers.Embedding(input_dim=vocab_sz, output_dim=embed_sz, input_length=input_length, trainable=True)\n",
    "        elif run_mode == 'vectorizer':\n",
    "            # Use a pre-trained embedding as a static vectorizer\n",
    "            self.embedding = tf.keras.layers.Embedding(input_dim=vocab_sz, output_dim=embed_sz, input_length=input_length, weights=[embedding_weights], trainable=False)\n",
    "        else:  # finetuning\n",
    "            # Fine-tune the pre-trained embedding\n",
    "            self.embedding = tf.keras.layers.Embedding(input_dim=vocab_sz, output_dim=embed_sz, input_length=input_length, weights=[embedding_weights], trainable=True)\n",
    "        \n",
    "        # Additional layers of the model\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_sz, activation='relu')  # Convolutional layer\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)  # Dropout layer to prevent overfitting\n",
    "        self.pool = tf.keras.layers.GlobalMaxPool1D()  # Global max pooling layer\n",
    "        self.dense = tf.keras.layers.Dense(output_sz, activation='softmax')  # Dense layer for classification\n",
    "\n",
    "    def call(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the SpamClassifierModel with specified parameters\n",
    "conv_num_filters = 256  # Number of convolutional filters\n",
    "conv_kernel_size = 3   # Size of the convolutional kernel\n",
    "model = SpamClassifierModel(vocab_size, EMBEDDING_DIM, max_seqlen, conv_num_filters, conv_kernel_size, NUM_CLASSES, 'scratch', E)\n",
    "\n",
    "# Build and compile the model\n",
    "model.build(input_shape=(None, max_seqlen))\n",
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model on the training dataset\n",
    "NUM_EPOCH = 3  # Number of training epochs\n",
    "CLASS_WEIGHTS = {0:1, 1:8}  # Class weights to handle class imbalance\n",
    "model.fit(train_dataset, epochs=NUM_EPOCH, validation_data=val_dataset, class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest_.tolist()) \n",
    "\n",
    "# Calculate and display test accuracy and confusion matrix\n",
    "if len(labels) == len(predictions):\n",
    "    print('Test accuracy: {:3f}'.format(accuracy_score(labels, predictions)))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(labels, predictions))\n",
    "else:\n",
    "    print('Error: Mismatch in the sizes of labels and predictions lists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7bc7c-1a36-418a-85fe-5a48610f642c",
   "metadata": {},
   "source": [
    "# Spam Prediction Function\n",
    "\n",
    "To utilize the trained model for practical applications, a function is defined to predict whether a given message is spam. This function handles the tokenization and sequence padding of the input message, and then uses the model to make a prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e03daab-7111-4d71-9888-21fdfab9f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "Spam\n"
     ]
    }
   ],
   "source": [
    "def predict_spam(message, model, tokenizer, max_seqlen):\n",
    "    # Tokenize and convert the message into a sequence of indices\n",
    "    sequence = tokenizer.texts_to_sequences([message])\n",
    "    \n",
    "    # Pad the sequence to ensure it has the same length as the model's input\n",
    "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_seqlen)\n",
    "    \n",
    "    # Make a prediction using the model\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    \n",
    "    # Determine the class (spam or not spam) based on the model's prediction\n",
    "    spam_prediction = prediction[0][1]  # Assuming the classes are [not spam, spam]\n",
    "    \n",
    "    return \"Spam\" if spam_prediction > 0.5 else \"Not Spam\"\n",
    "\n",
    "# Example usage of the function\n",
    "message_to_check = \"Hi, this is Cynde from HR. We have a couple question regarding your application. Please call [number] to schedule a interview.\"\n",
    "print(predict_spam(message_to_check, model, tokenizer, max_seqlen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf934f96-3e76-429f-bcdd-43bd3c269fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
